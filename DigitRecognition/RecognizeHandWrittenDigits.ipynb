{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-Written Digits Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load MNIST images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import mnist_loader\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, layers):\n",
    "        \"\"\" \n",
    "        layers is a list. length of layers is the number of layers. each entry in the \n",
    "        list denotes the number of neurons in each layer\n",
    "        layers = [num neurons in layer 0, num neurons in layer 1, num neurons in layer 2, ...]        \n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.numLayers = len(layers)\n",
    "        \n",
    "        # initialize weights = [[from all input to neuron 0], [from all input to neuron 1], ...]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.layers[:-1], self.layers[1:])]\n",
    "        # initialize biases = [none, vector of all layer 1 neurons, vector of all layer 2 neurons, ...]\n",
    "        # every layer has one bias\n",
    "        self.biases  = [np.random.randn(y, 1) for y in self.layers[1:]]\n",
    "        # Constants for speeding up \n",
    "        self.weightGradZeroed = [np.zeros(w.shape) for w in self.weights]\n",
    "        self.biasGradZeroed = [np.zeros(b.shape) for b in self.biases]\n",
    "   \n",
    "    def costDerivative(self, aL, y):\n",
    "        \"\"\"\n",
    "        compute gradient of Quadratic Cost function w.r.t activation of output layer\n",
    "        partial derivative C / partial derivative aL = aL - y\n",
    "        \"\"\"\n",
    "        return (aL - y)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid Activation Function\n",
    "        \"\"\"\n",
    "        return (1.0/(1.0+np.exp(-z)))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function\n",
    "        \"\"\"\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "    def ff(self, a):\n",
    "        \"\"\"\n",
    "        Return the output of the network if ``a`` is input\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def feedForward(self, x):\n",
    "        \"\"\"\n",
    "        1. compute weighted input z for every neuron from layer 2 to L\n",
    "        2. compute activation a = sigmoid(z) for every neuron from layer 2 to L\n",
    "        \"\"\"\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        # zs is a list of all z in all layers\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        return activations, zs\n",
    "            \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        1. Call feedForward to compute all activations and weighted input in every layer\n",
    "        2. Compute gradients of weights and biases of every layer\n",
    "        \"\"\"    \n",
    "\n",
    "        # call feed forward to compute z and a of every layer. \n",
    "        # input x is the activation of layer 0\n",
    "        activations, zs = self.feedForward(x)\n",
    "        \n",
    "        bpBiasGrad = [np.zeros(b.shape) for b in self.biases]\n",
    "        bpWeightGrad = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Last layer L: error (delta) and weights\n",
    "        delta = self.costDerivative(activations[-1], y) * self.sigmoid_prime(zs[-1])\n",
    "        weights = np.dot(delta, activations[-2].transpose())\n",
    "        bpBiasGrad[-1] = delta\n",
    "        bpWeightGrad[-1] = weights\n",
    "        \n",
    "        # back propagate error, starting from the penultimate layer\n",
    "        for l in range(1, self.numLayers-1):\n",
    "            sp = self.sigmoid_prime(zs[-1-l])\n",
    "            delta = np.dot(self.weights[-l].transpose(), delta) * sp\n",
    "            weights = np.dot(delta, activations[-2-l].transpose())\n",
    "            bpBiasGrad[-1-l] = delta\n",
    "            bpWeightGrad[-1-l] = weights\n",
    "            \n",
    "        return bpBiasGrad, bpWeightGrad, activations\n",
    "\n",
    "  \n",
    "    def updateMiniBatch(self, mini_batch, lr):\n",
    "        \"\"\"\n",
    "        Iterate through every sample in a mini-batch. mini_batch is a tuple of (x, y)\n",
    "            1. Call backprop\n",
    "            2. Accumulate gradients from sample to sample\n",
    "\n",
    "        At the end of one mini-batch\n",
    "            1. Updated weights = previous weights - scaled learning rate * accumulated weights\n",
    "            2. Update biases = previous bias - scaled learning rate * accumulated biases\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize bias and weight accumulators\n",
    "        \n",
    "        #weightGradAccum = [np.zeros(w.shape) for w in self.weights]\n",
    "        #biasGradAccum = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # zero out the accumulators\n",
    "        weightGradAccum = self.weightGradZeroed\n",
    "        biasGradAccum   = self.biasGradZeroed\n",
    "        \n",
    "        # iterate every sample in the mini-batch\n",
    "        for x, y in mini_batch:\n",
    "            # backprop returns computed gradients of biases and weights of one sample\n",
    "            biasGrad, weightGrad, activations = self.backprop(x, y)\n",
    "            # biasAccum = [biasAccum + biasGrad] ??\n",
    "            biasGradAccum   = [ba+bg for ba, bg in zip(biasGradAccum, biasGrad)]\n",
    "            weightGradAccum = [wa+wg for wa, wg in zip(weightGradAccum, weightGrad)]\n",
    "        \n",
    "        # compute effective learning rate \n",
    "        effectiveLr = lr/len(mini_batch)\n",
    "        \n",
    "        # update biases and weights\n",
    "        self.biases = [b - bg * effectiveLr for b, bg in zip(self.biases, biasGradAccum)]\n",
    "        self.weights = [w - wg * effectiveLr for w, wg in zip(self.weights, weightGradAccum)]\n",
    "        \n",
    "\n",
    "    def train(self, training_data, epochs, mini_batch_size, lr, test_data=None):\n",
    "        \"\"\"\n",
    "        run N epochs\n",
    "          1. shuffle training data\n",
    "          2. create a list of mini-batches from training data\n",
    "          3. take each mini-batch and call updateMiniBatch \n",
    "          4. update weights and biases after each mini-batch\n",
    "          5. If there is test data, call evaluate with test data and print result\n",
    "             else print Epoch number complete\n",
    "        \"\"\"\n",
    "        # cast training_data from zip object to list of zipped pairs\n",
    "        training_data_list = list(training_data)\n",
    "        n = len(training_data_list)\n",
    "        \n",
    "        test_data_list = list(test_data)\n",
    "        n_test = len(test_data_list)\n",
    "        \n",
    "        for iEpoch in range (epochs):\n",
    "            np.random.shuffle(training_data_list)\n",
    "            mini_batches = [training_data_list[iSample:iSample+mini_batch_size] for iSample in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            # run one mini-batch at a time until all mini-batches are processed\n",
    "            for mini_batch in mini_batches:\n",
    "                self.updateMiniBatch(mini_batch, lr)\n",
    "                \n",
    "            print('========= finished training ======')\n",
    "            if test_data:\n",
    "                test_results = self.evaluate(test_data_list)\n",
    "                print (\"Epoch {0}: {1} / {2}\".format(\n",
    "                    iEpoch, test_results, n_test))\n",
    "            else:\n",
    "                print ('Epoch {0} complete'.format(iEpoch))\n",
    "                \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs where the neural\n",
    "        network outputs the correct result. Neural\n",
    "        network's output is the index of the neuron in the final layer \n",
    "        that has the highest activation.\"\"\"\n",
    "        \n",
    "        test_results = [(np.argmax(self.ff(x)),y)\n",
    "                        for (x, y) in test_data]\n",
    "\n",
    "        return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= finished training ======\n",
      "Epoch 0: 7922 / 10000\n",
      "========= finished training ======\n",
      "Epoch 1: 8596 / 10000\n",
      "========= finished training ======\n",
      "Epoch 2: 8803 / 10000\n",
      "========= finished training ======\n",
      "Epoch 3: 8897 / 10000\n",
      "========= finished training ======\n",
      "Epoch 4: 8949 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 16, 10])\n",
    "net.train(training_data, 5, 64, lr=3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
